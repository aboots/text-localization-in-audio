{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we want to use a pre-trained ASR model form huggingface to get transcripts of each chunks that are segmented and stored before."
      ],
      "metadata": {
        "id": "u3w0KQ3yD6S7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPwHLAdCKVXa"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4RO5Ml6KXsl"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "import torchaudio\n",
        "import torchaudio.transforms as transforms\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jE7Cq7E5KsIL"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"m3hrdadfi/wav2vec2-large-xlsr-persian\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"m3hrdadfi/wav2vec2-large-xlsr-persian\").eval().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VKRq73j9KxL0"
      },
      "outputs": [],
      "source": [
        "prefix_url = 'https://traffic.libsyn.com/secure/radiomarz/'\n",
        "\n",
        "with open('./persian_dataset_chunks.json', 'r') as inFile:\n",
        "  dataset = json.load(inFile)\n",
        "\n",
        "utterances =  list(dataset.keys())\n",
        "\n",
        "for utterance in utterances:\n",
        "\n",
        "    if not os.path.exists(utterance):\n",
        "        !wget '{prefix_url}{utterance}'\n",
        "\n",
        "    waveform, sr = torchaudio.load(utterance)\n",
        "\n",
        "    sample_rate = 16000\n",
        "    resample_transform = transforms.Resample(sr,sample_rate)\n",
        "    waveform = resample_transform(waveform).squeeze().numpy()\n",
        "    \n",
        "    for i, chunk in enumerate(dataset[utterance]):\n",
        "        \n",
        "        start_time = chunk[\"start_time\"]\n",
        "        end_time = chunk[\"end_time\"]\n",
        "        \n",
        "\n",
        "        #Skip if transcript already obtained\n",
        "        if 'transcription' in chunk.keys():\n",
        "            continue \n",
        "            \n",
        "        start_sample = int(start_time * sample_rate)\n",
        "        end_sample = int(end_time * sample_rate)\n",
        "        audio_segment = waveform[start_sample:end_sample]\n",
        "\n",
        "        input_values = processor(audio_segment, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n",
        "        input_values = input_values.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_values).logits\n",
        "\n",
        "        predicted_ids = np.argmax(logits.cpu().detach().numpy(), axis=-1)\n",
        "        transcription = processor.decode(predicted_ids[0])\n",
        "        \n",
        "        dataset[utterance][i][\"transcription\"] = transcription\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHaqUeEOsLoN"
      },
      "outputs": [],
      "source": [
        "with open(\"persian_dataset.json\", \"w\", encoding='utf-8') as inFile:\n",
        "  json.dump(dataset,inFile,indent=1,ensure_ascii=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}