{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3v6nNNBKGfp"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPwHLAdCKVXa"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/datasets.git\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install -U sentence-transformers\n",
        "!pip install torchaudio\n",
        "!pip install librosa\n",
        "!pip install jiwer\n",
        "!pip install hazm\n",
        "!pip install yake\n",
        "!pip install multi_rake\n",
        "!pip install fasttext\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n",
        "!gzip -d cc.fa.300.bin.gz\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "sj-3lD8oQR6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4RO5Ml6KXsl"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import os\n",
        "import gc\n",
        "import json\n",
        "import torch\n",
        "import hazm\n",
        "import yake\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "from multi_rake import Rake\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "from sentence_transformers import models, SentenceTransformer, util"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ASR & Keyword Extraction Section\n"
      ],
      "metadata": {
        "id": "CqaoHSRvSaoy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jE7Cq7E5KsIL"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"m3hrdadfi/wav2vec2-large-xlsr-persian\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"m3hrdadfi/wav2vec2-large-xlsr-persian\").to(device)\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKRq73j9KxL0"
      },
      "outputs": [],
      "source": [
        "def transcribe_dataset(dataset):\n",
        "    utterances = dataset.keys()\n",
        "    for utterance in utterances:\n",
        "        waveform, sample_rate = librosa.load(utterance, sr=16000)\n",
        "        for chunk in dataset[utterance]:\n",
        "            start_time = chunk[\"start_time\"]\n",
        "            end_time = chunk[\"end_time\"]\n",
        "            start_sample = int(start_time * sample_rate)\n",
        "            end_sample = int(end_time * sample_rate)\n",
        "            audio_segment = waveform[start_sample:end_sample]\n",
        "\n",
        "            input_values = processor(audio_segment, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n",
        "            input_values = input_values.to(device)\n",
        "            logits = model(input_values).logits\n",
        "            predicted_ids = np.argmax(logits.cpu().detach().numpy(), axis=-1)\n",
        "            transcription = processor.decode(predicted_ids[0])\n",
        "            \n",
        "            chunk[\"transcription\"] = transcription\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keywords_yake(transcription):\n",
        "    # Tokenize the transcription using Hazm\n",
        "    normalizer = hazm.Normalizer()\n",
        "    tokenizer = hazm.WordTokenizer()\n",
        "    tokens = tokenizer.tokenize(normalizer.normalize(transcription))\n",
        "    tokens = [token for token in tokens if token not in hazm.stopwords_list()]\n",
        "\n",
        "    # Extract keywords using YAKE\n",
        "    extractor = yake.KeywordExtractor()\n",
        "    keywords = extractor.extract_keywords(' '.join(tokens))\n",
        "    keywords = [k[0] for k in keywords]\n",
        "    keywords = list(map(lambda x: x.replace('\\u200c', ''), keywords))\n",
        "\n",
        "    return keywords\n",
        "\n",
        "\n",
        "def extract_keywords_rake(transcription):\n",
        "    # Extract keywords using multi_rake\n",
        "    stopwords = hazm.stopwords_list()\n",
        "    rake = Rake(\n",
        "        min_chars=3,\n",
        "        max_words=3,\n",
        "        min_freq=1,\n",
        "        language_code=None,  \n",
        "        stopwords=None, \n",
        "        lang_detect_threshold=50,\n",
        "        max_words_unknown_lang=2,\n",
        "        generated_stopwords_percentile=80,\n",
        "        generated_stopwords_max_len=3,\n",
        "        generated_stopwords_min_freq=2,\n",
        "    )\n",
        "    keywords = rake.apply(transcription)\n",
        "    keywords = [kw[0].replace('\\u200c', '') for kw in keywords]\n",
        "\n",
        "    return keywords"
      ],
      "metadata": {
        "id": "eHXufIxWtV96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataset_keywords(dataset_file):\n",
        "    with open(dataset_file, 'r') as f:\n",
        "        dataset = json.load(f)  \n",
        "    \n",
        "    dataset = transcribe_dataset(dataset)\n",
        "    utterances = dataset.keys()\n",
        "    for utterance in utterances:\n",
        "        for chunk in dataset[utterance]:\n",
        "            chunk[\"keywords\"] = extract_keywords_yake(chunk[\"transcription\"])\n",
        "\n",
        "    transcribed_dataset_file = 'transcribed_' + dataset_file\n",
        "    with open(transcribed_dataset_file, \"w\", encoding='utf-8') as outFile:\n",
        "        json.dump(dataset, outFile, ensure_ascii=False, indent=1)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "VcrP42TZ2bf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query and Dataset Comparison Section"
      ],
      "metadata": {
        "id": "iIJBRcY5Slz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = sum([vec1[i] * vec2[i] for i in range(len(vec1))])\n",
        "    norm1 = sum([vec1[i] ** 2 for i in range(len(vec1))]) ** 0.5\n",
        "    norm2 = sum([vec2[i] ** 2 for i in range(len(vec2))]) ** 0.5\n",
        "    return dot_product / (norm1 * norm2)"
      ],
      "metadata": {
        "id": "65O1-aNwQJPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_st_model(model_name_or_path):\n",
        "    word_embedding_model = models.Transformer(model_name_or_path)\n",
        "    pooling_model = models.Pooling(\n",
        "        word_embedding_model.get_word_embedding_dimension(),\n",
        "        pooling_mode_mean_tokens=True,\n",
        "        pooling_mode_cls_token=False,\n",
        "        pooling_mode_max_tokens=False)\n",
        "    \n",
        "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_results(dataset_file, query, N):\n",
        "\n",
        "    with open(dataset_file, 'r') as f:\n",
        "        dataset = json.load(f)\n",
        "\n",
        "    st_model = load_st_model('m3hrdadfi/bert-fa-base-uncased-wikitriplet-mean-tokens')\n",
        "    query_keywords = extract_keywords_yake(query)\n",
        "\n",
        "    # Compute average embedding of query keywords\n",
        "    query_embedding = st_model.encode(query)\n",
        "\n",
        "    similarity_scores = []\n",
        "    for utterance in dataset.keys():\n",
        "        for chunk in dataset[utterance]:\n",
        "            chunk_keywords = ' '.join(chunk['keywords'])\n",
        "            chunk_embedding = st_model.encode(chunk_keywords)\n",
        "            similarity_scores.append((utterance, chunk['start_time'], chunk['end_time'],\n",
        "                                      chunk['transcription'], cosine_similarity(query_embedding, chunk_embedding)))\n",
        "            \n",
        "    # Sort similarity scores in descending order\n",
        "    similarity_scores.sort(key=lambda x: x[4], reverse=True)\n",
        "\n",
        "    # Get top N most similar records\n",
        "    return similarity_scores[:N]"
      ],
      "metadata": {
        "id": "qtlXLkc3R3A4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def usage(dataset, query, N):\n",
        "    if not os.path.isfile(os.getcwd() + 'transcribed_' + dataset):\n",
        "        print(\"Generating Dataset Keywords...\")\n",
        "        generate_dataset_keywords('dataset.json')\n",
        "\n",
        "    return get_results('transcribed_' + dataset, query, N)"
      ],
      "metadata": {
        "id": "tmratX3mUvT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"جهان آغازین\"\n",
        "usage('dataset.json', query, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prKGqzciXJ53",
        "outputId": "4cc5bc37-2501-4fb2-d089-af8a8e5d3073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating Dataset Keywords...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('sina-farsi.wav',\n",
              "  14.5,\n",
              "  23,\n",
              "  'ستاما جهان آغازین فضایی عجیبتر با ستارههای غولپیکرد بود که به سرعت زندگی میکردند و در سن جوانی میمردند',\n",
              "  0.5871606050725281),\n",
              " ('sina-farsi.wav',\n",
              "  0.0,\n",
              "  14.5,\n",
              "  'بر اساس پژبوهشی جدید اولین ستارههای کیهان تا بیش از ده هزار برابر جرم خورشید رشد کردند و هزار برابر بزرگتر از بزرگترین ستارههای کنونی بودند امروزه جرم بزرگترین ستارهها صد جرم خورشید',\n",
              "  0.3525935654811955),\n",
              " ('sina-farsi.wav',\n",
              "  23,\n",
              "  30,\n",
              "  'با مرگ این الماسهاید رخشان دیگر هرگز شرایط شکلگیری آنها فراهم نشد',\n",
              "  0.25459224942676356)]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cTOfI8p5cDlX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}