{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-03-10T16:19:26.443966Z","iopub.status.busy":"2023-03-10T16:19:26.443379Z","iopub.status.idle":"2023-03-10T16:19:38.239051Z","shell.execute_reply":"2023-03-10T16:19:38.237708Z","shell.execute_reply.started":"2023-03-10T16:19:26.443924Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.26.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T16:31:30.479774Z","iopub.status.busy":"2023-03-10T16:31:30.479276Z","iopub.status.idle":"2023-03-10T16:31:30.488273Z","shell.execute_reply":"2023-03-10T16:31:30.487172Z","shell.execute_reply.started":"2023-03-10T16:31:30.479731Z"},"trusted":true},"outputs":[],"source":["import json\n","import torch\n","from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n","from nltk import sent_tokenize, word_tokenize\n","from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n","import torch.nn as nn\n","import re\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","from tqdm import tqdm\n","import gc"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-03-03T15:04:56.378458Z","iopub.status.busy":"2023-03-03T15:04:56.378053Z","iopub.status.idle":"2023-03-03T15:04:56.423816Z","shell.execute_reply":"2023-03-03T15:04:56.422815Z","shell.execute_reply.started":"2023-03-03T15:04:56.378424Z"},"trusted":true},"outputs":[],"source":["data_file = \"/kaggle/input/persian-keywords/PersianNewsDataset.txt\"\n","def read_data(file_path):\n","    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","        data = [json.loads(line) for line in f]\n","    return data\n","data = read_data(data_file)"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-03-03T15:05:02.558899Z","iopub.status.busy":"2023-03-03T15:05:02.558537Z","iopub.status.idle":"2023-03-03T15:05:02.566628Z","shell.execute_reply":"2023-03-03T15:05:02.565430Z","shell.execute_reply.started":"2023-03-03T15:05:02.558865Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'_  در حالی که انتشار خبرها درباره تیراندازی به ساختمان یوتیوب ادامه دارد، بررسی اینترنتی سوابق «نسیم نجفی اقدم» مظنون اصلی این حمله حاکی از احتمال «بهایی» بودن او است. بامداد امروز چهارشنبه در ابتدا منابع خبری از تیراندازی در ساختمان مقر مرکزی سایت معروف یوتیوب خبر دادند. ساعاتی پس از انتشار خبر اولیه این تیراندازی، نام «نسیم نجفی اقدم» به عنوان ضارب توسط رسانه\\u200cهای آمریکایی اعلام شد (جزئیات بیشتر) بنا بر اعلام رسانه\\u200cهای آمریکایی، وی از مواضع و سیاست\\u200cهای یوتیوب به شدت انتقاد داشت و پدرش هم گفته بود که او «از یوتیوب متنفر بود». طبق اعلام رسانه\\u200cها و مقام\\u200cهای آمریکایی، ضارب پس از زخمی کردن سه نفر، به خودش تیراندازی و خودکشی کرد. با انجام جستجوهای اینترنتی درباره «نسیم نجفی اقدم»، برخی منابع اطلاعاتی درباره او منتشر کرده\\u200cاند؛ اطلاعاتی که صحت آن قابل تایید و یا تکذیب قطعی نیست. بنا بر اطلاعات برخی از این منابع از قبیل سایت heavy.com، نسیم نجفی اقدم، معتقد به فرقه ضاله «بهائیت» بوده است. او همچنین در برخی از شبکه\\u200cهای اجتماعی، مطالبی درباره این فرقه ضاله \\xa0هم منتشر و خود را پایبند به آن اعلام کرده بود. سایت «بازفید» هم در این باره گزارشی منتشر کرد. طبق این گزارش که برگرفته از وبسایت خود اقدم است، او از دیدارش با اعضای این فرقه ضاله در سال 2015 خبر داده است. با نگاه به مطالب منتشر شده در شبکه\\u200cهای اجتماعی و حتی سایت شخصی او، باید گفت اگر هم وی، خودش عضو فرقه ضاله بهائیت نبوده باشد اما تحت تاثیر تفکرات این فرقه قرار داشت. شبکه اسکای نیوز در گزارشی درباره عامل تیراندازی به ساختمان یوتیوب، جزئیات دیگری منتشر کرد. طبق گزارش این رسانه، اقدم به همراه خانواده خود در سال 1996 میلادی از ایران به آمریکا مهاجرت کرده بود. او به صورت مرتب در یوتیوب پُست\\u200cها و ویدئوهایی به زبان\\u200cهای انگلیسی، فارسی و ترکیه\\u200cای منتشر می\\u200cکرد'"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["data[0]['body']"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2023-03-03T15:05:09.538183Z","iopub.status.busy":"2023-03-03T15:05:09.537813Z","iopub.status.idle":"2023-03-03T15:05:09.544549Z","shell.execute_reply":"2023-03-03T15:05:09.543423Z","shell.execute_reply.started":"2023-03-03T15:05:09.538151Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['یوتیوب', 'ساختمان یوتیوب', 'نسیم نجفی اقدم', 'بهایی']"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["data[0]['keywords']"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2023-03-03T15:09:38.892881Z","iopub.status.busy":"2023-03-03T15:09:38.892401Z","iopub.status.idle":"2023-03-03T15:09:38.901461Z","shell.execute_reply":"2023-03-03T15:09:38.900379Z","shell.execute_reply.started":"2023-03-03T15:09:38.892839Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2023-03-03T16:27:01.677079Z","iopub.status.busy":"2023-03-03T16:27:01.676672Z","iopub.status.idle":"2023-03-03T16:27:04.108436Z","shell.execute_reply":"2023-03-03T16:27:04.107237Z","shell.execute_reply.started":"2023-03-03T16:27:01.677033Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at HooshvareLab/bert-fa-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["[]\n"]}],"source":["# load the tokenizer and pre-trained model for Persian BERT\n","tokenizer = BertTokenizer.from_pretrained('HooshvareLab/bert-fa-base-uncased', do_lower_case=True)\n","model = BertModel.from_pretrained('HooshvareLab/bert-fa-base-uncased').to(device)\n","\n","# define the function to extract keywords from a text document\n","def extract_keywords(text):\n","    # tokenize the input text and convert to input IDs and attention mask\n","    inputs = tokenizer.encode_plus(\n","        text,\n","        add_special_tokens=True,\n","        return_token_type_ids=False,\n","        padding='max_length',\n","        truncation=True,\n","        max_length=128,\n","        return_attention_mask=True,\n","        return_tensors='pt'\n","    )\n","    input_ids = inputs['input_ids'].to(device)\n","    attention_mask = inputs['attention_mask'].to(device)\n","\n","    # get the BERT embeddings for the input text\n","    with torch.no_grad():\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","    embeddings = outputs.last_hidden_state.cpu().numpy()[0]\n","\n","    # calculate the vector representation of each token by averaging its BERT embeddings\n","    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n","    token_vectors = []\n","    for i in range(len(tokens)):\n","        if tokens[i] != '[PAD]':\n","            token_vectors.append(embeddings[i])\n","    token_vectors = torch.tensor(token_vectors).to(device)\n","    vector_means = torch.mean(token_vectors, dim=0)\n","\n","    # get the keywords by finding the tokens with the highest cosine similarity to the vector mean\n","    similarities = torch.nn.functional.cosine_similarity(token_vectors, vector_means, dim=1).cpu().numpy()\n","    keyword_indices = similarities.argsort()[-10:][::-1] # get top 5 keywords\n","    keywords = []\n","    for i in keyword_indices:\n","        if similarities[i] > 0.7: # threshold for keyword selection\n","            if tokens[i] == '[CLS]':\n","                continue\n","            keywords.append(tokens[i])\n","            if len(keywords) == 5:\n","                break\n","    return keywords\n","\n","# example usage\n","text = \"این روزها همه چیز درباره ویروس کروناست. اما آیا این ویروس واقعا خطرناک است؟\"\n","keywords = extract_keywords(text)\n","print(keywords)\n"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2023-03-03T16:27:40.970954Z","iopub.status.busy":"2023-03-03T16:27:40.970430Z","iopub.status.idle":"2023-03-03T16:27:41.221541Z","shell.execute_reply":"2023-03-03T16:27:41.219095Z","shell.execute_reply.started":"2023-03-03T16:27:40.970920Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['او', 'کرد', 'سایت', 'کرده', 'بود', 'میکرد', 'او', 'منتشر']\n"]}],"source":["keywords = []\n","for sentence in sent_tokenize(data[0]['body']):\n","    keywords.extend(extract_keywords(sentence))\n","print(keywords)"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2023-03-03T16:28:07.556826Z","iopub.status.busy":"2023-03-03T16:28:07.555861Z","iopub.status.idle":"2023-03-03T16:28:07.565690Z","shell.execute_reply":"2023-03-03T16:28:07.564495Z","shell.execute_reply.started":"2023-03-03T16:28:07.556787Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['یوتیوب', 'ساختمان یوتیوب', 'نسیم نجفی اقدم', 'بهایی']"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":["data[0]['keywords']"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2023-03-03T17:05:25.889379Z","iopub.status.busy":"2023-03-03T17:05:25.888925Z","iopub.status.idle":"2023-03-03T17:05:25.897850Z","shell.execute_reply":"2023-03-03T17:05:25.896813Z","shell.execute_reply.started":"2023-03-03T17:05:25.889340Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModel\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchcrf import CRF\n","\n","# load pre-trained BERT model and tokenizer\n","model_name = \"HooshvareLab/bert-fa-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","bert_model = AutoModel.from_pretrained(model_name)\n","\n","# define the IOB labels\n","iob_labels = [\"O\", \"B-KEYWORD\", \"I-KEYWORD\"]\n","\n","# define the model architecture\n","class BertForKeywordExtraction(nn.Module):\n","    def __init__(self, bert_model, num_labels):\n","        super(BertForKeywordExtraction, self).__init__()\n","        self.bert = bert_model\n","        self.dropout = nn.Dropout(0.1)\n","        self.fc = nn.Linear(768, num_labels)\n","        self.crf = CRF(num_labels)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","        sequence_output = outputs.last_hidden_state\n","        sequence_output = self.dropout(sequence_output)\n","        sequence_logits = self.fc(sequence_output)\n","        sequence_tags = self.crf.decode(sequence_logits, attention_mask)\n","        return sequence_tags\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install transformers\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","\n","# Define a custom dataset for your annotated data\n","class CustomDataset(Dataset):\n","    def __init__(self, data, tokenizer):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        \n","    def __getitem__(self, idx):\n","        text = self.data[idx]['text']\n","        keywords = self.data[idx]['keywords']\n","        label = [1 if keyword in text else 0 for keyword in keywords] # generate one-hot encoded labels\n","        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n","        return {'input_ids': encoding['input_ids'].squeeze(0), \n","                'attention_mask': encoding['attention_mask'].squeeze(0), \n","                'labels': torch.tensor(label)}\n","    \n","    def __len__(self):\n","        return len(self.data)\n","    \n","\n","# Load your annotated dataset\n","data = [{\"text\": \"This is a sample document containing some keywords.\", \"keywords\": [\"sample\", \"keywords\"]},\n","        {\"text\": \"Another sample document with some important keywords.\", \"keywords\": [\"important\", \"keywords\"]},\n","        # more documents...\n","       ]\n","\n","# Define the pre-trained Persian BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('HooshvareLab/bert-fa-base-uncased')\n","\n","# Instantiate your custom dataset\n","dataset = CustomDataset(data, tokenizer)\n","\n","# Define the data loader\n","data_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n","\n","# Load the pre-trained Persian BERT model for sequence classification task\n","model = BertForSequenceClassification.from_pretrained('HooshvareLab/bert-fa-base-uncased', num_labels=len(data[0]['keywords']))\n","\n","# Define the optimizer\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","# Define the loss function\n","loss_fn = torch.nn.BCEWithLogitsLoss()\n","\n","# Train the model on your annotated dataset\n","epochs = 5\n","for epoch in range(epochs):\n","    for batch in data_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        \n","        optimizer.zero_grad()\n","        \n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = loss_fn(outputs.logits, labels.float())\n","        loss.backward()\n","        optimizer.step()\n","        \n","    print('Epoch:', epoch+1, 'Loss:', loss.item())\n","\n","# Save the fine-tuned model\n","model.save_pretrained('fine_tuned_bert_model')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the fine-tuned model\n","model = BertForSequenceClassification.from_pretrained('fine_tuned_bert_model')\n","tokenizer = BertTokenizer.from_pretrained('HooshvareLab/bert-fa-base-uncased')\n","\n","# Define a function for predicting the keywords of a new sentence\n","def predict_keywords(sentence, threshold=0.5):\n","    encoding = tokenizer(sentence, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n","    outputs = model(encoding['input_ids'].to(device), attention_mask=encoding['attention_mask'].to(device))\n","    probs = torch.sigmoid(outputs.logits).\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from transformers import BertTokenizer, BertForTokenClassification\n","\n","# Load the fine-tuned model\n","model = BertForTokenClassification.from_pretrained('path/to/fine/tuned/model')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","\n","def extract_keywords(text):\n","    # Tokenize the text\n","    tokenized_text = tokenizer.encode(text, add_special_tokens=True)\n","    # Convert token ids to tensor\n","    input_ids = torch.tensor([tokenized_text])\n","    # Get the model output\n","    with torch.no_grad():\n","        output = model(input_ids)\n","    # Get the predicted label ids\n","    label_ids = torch.argmax(output[0], axis=2)[0]\n","    # Convert the label ids to labels\n","    labels = [tokenizer.decode([label_id]) for label_id in label_ids]\n","    # Get the keywords\n","    keywords = []\n","    for i in range(len(tokenized_text)):\n","        if labels[i] == 'B-KW':\n","            keyword = tokenizer.decode([tokenized_text[i]])\n","            # Add the keyword to the list\n","            keywords.append(keyword)\n","        elif labels[i] == 'I-KW':\n","            keyword += ' ' + tokenizer.decode([tokenized_text[i]])\n","            # Update the last keyword in the list\n","            keywords[-1] = keyword\n","    return keywords\n"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2023-03-03T16:17:24.180202Z","iopub.status.busy":"2023-03-03T16:17:24.179237Z","iopub.status.idle":"2023-03-03T16:17:24.203979Z","shell.execute_reply":"2023-03-03T16:17:24.202914Z","shell.execute_reply.started":"2023-03-03T16:17:24.180149Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['وی', 'او', 'شد']\n"]}],"source":["\n","train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","from transformers import BertTokenizer, BertForSequenceClassification\n","\n","# load the tokenizer and fine-tuned model for Persian BERT\n","tokenizer = BertTokenizer.from_pretrained('HooshvareLab/bert-fa-base-uncased', do_lower_case=True)\n","model = BertForSequenceClassification.from_pretrained('path/to/fine-tuned/model', num_labels=num_keywords).to(device)\n","\n","# define the function to extract keywords from a text document\n","def extract_keywords(text):\n","    # tokenize the input text and convert to input IDs and attention mask\n","    inputs = tokenizer.encode_plus(\n","        text,\n","        add_special_tokens=True,\n","        return_token_type_ids=False,\n","        padding='max_length',\n","        truncation=True,\n","        max_length=128,\n","        return_attention_mask=True,\n","        return_tensors='pt'\n","    )\n","    input_ids = inputs['input_ids'].to(device)\n","    attention_mask = inputs['attention_mask'].to(device)\n","\n","    # run the model on the input text and get the predicted keyword probabilities\n","    with torch.no_grad():\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","    logits = outputs.logits\n","    probabilities = torch.sigmoid(logits).cpu().numpy()[0]\n","\n","    # get the keywords with the highest probability\n","    keyword_indices = probabilities.argsort()[-5:][::-1] # get top 5 keywords\n","    keywords = []\n","    for i in keyword_indices:\n","        if probabilities[i] > 0.5: # threshold for keyword selection\n","            keywords.append(tokenizer.decode(i))\n","    return keywords\n","\n","# example usage\n","text = \"این روزها همه چیز درباره ویروس کروناست. اما آیا این ویروس واقعا خطرناک است؟\"\n","keywords = extract_keywords(text)\n","print(keywords)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the pre-trained ParsBERT model and tokenizer\n","model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=2).to\n","\n","\n","# Define the fine-tuning task\n","label_list = [\"O\", \"B-KEYWORD\", \"I-KEYWORD\"]\n","\n","def encode_tags(tags, max_length):\n","    encoded = [1] * max_length\n","    for i, tag in enumerate(tags):\n","        if tag == \"O\":\n","            encoded[i] = 0\n","        elif tag.startswith(\"B-\"):\n","            encoded[i] = 1\n","        elif tag.startswith(\"I-\"):\n","            encoded[i] = 2\n","    return encoded\n","\n","def preprocess_data(data):\n","    input_ids = []\n","    attention_masks = []\n","    tags = []\n","    for item in data:\n","        text = f\"{item['title']} {item['lead']} {item['body']}\"\n","        encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512)\n","        input_ids.append(encoding[\"input_ids\"])\n","        attention_masks.append(encoding[\"attention_mask\"])\n","        tags.append(encode_tags([\"O\"] * len(text.split()), 512))\n","    return {\n","        \"input_ids\": input_ids,\n","        \"attention_mask\": attention_masks,\n","        \"tags\": tags\n","    }\n","\n","data = preprocess_data(data)\n","\n","# Split the data into training, validation, and testing sets\n","train_data = data[:int(0.8*len(data))]\n","dev_data = data[int(0.8*len(data)):int(0.9*len(data))]\n","test_data = data[int(0.9*len(data)):]\n","\n","# Fine-tune the model\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    push_to_hub=False,\n","    logging_dir=\"./logs\",\n","    logging_steps=10,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Summarizer"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T16:29:01.560109Z","iopub.status.busy":"2023-03-10T16:29:01.559739Z","iopub.status.idle":"2023-03-10T16:29:01.574757Z","shell.execute_reply":"2023-03-10T16:29:01.573693Z","shell.execute_reply.started":"2023-03-10T16:29:01.560076Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-03-07T15:15:55.654100Z","iopub.status.busy":"2023-03-07T15:15:55.653618Z","iopub.status.idle":"2023-03-07T15:17:31.718232Z","shell.execute_reply":"2023-03-07T15:17:31.717130Z","shell.execute_reply.started":"2023-03-07T15:15:55.654056Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"69ffa9abbc9043e9ad14342b5b7917aa","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/375 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c3e9ca4bd6a44b869deb6e8cd29991a3","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/730 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"364932baa5ef422d8734a57757fa0805","version_major":2,"version_minor":0},"text/plain":["Downloading (…)\"spiece.model\";:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f894cd0884e2435d98666e479ec511bc","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py:447: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8968097e7ad246b989a432f3c353b50f","version_major":2,"version_minor":0},"text/plain":["Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_name = \"csebuetnlp/mT5_multilingual_XLSum\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T16:29:18.560524Z","iopub.status.busy":"2023-03-10T16:29:18.559529Z","iopub.status.idle":"2023-03-10T16:29:45.516892Z","shell.execute_reply":"2023-03-10T16:29:45.515644Z","shell.execute_reply.started":"2023-03-10T16:29:18.560487Z"},"trusted":true},"outputs":[],"source":["token='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'\n","tokenizer = AutoTokenizer.from_pretrained(\"arshandalili/autotrain-news-summarization-3366493100\", use_auth_token=token)\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"arshandalili/autotrain-news-summarization-3366493100\", use_auth_token=token).to(device)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T16:30:16.542244Z","iopub.status.busy":"2023-03-10T16:30:16.541748Z","iopub.status.idle":"2023-03-10T16:30:16.551852Z","shell.execute_reply":"2023-03-10T16:30:16.550848Z","shell.execute_reply.started":"2023-03-10T16:30:16.542198Z"},"trusted":true},"outputs":[],"source":["def summarize(text, model, tokenizer):\n","    WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n","\n","    input_ids = tokenizer(\n","        [WHITESPACE_HANDLER(text)],\n","        return_tensors=\"pt\",\n","        padding=\"max_length\",\n","        truncation=True,\n","        max_length=512\n","    )[\"input_ids\"].to(device)\n","\n","    output_ids = model.generate(\n","        input_ids=input_ids,\n","        no_repeat_ngram_size=2,\n","        max_length=84,\n","        num_beams=4\n","    )[0]\n","\n","    summary = tokenizer.decode(\n","        output_ids,\n","        skip_special_tokens=True,\n","        clean_up_tokenization_spaces=False\n","    )\n","    \n","    return summary"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T16:30:18.451080Z","iopub.status.busy":"2023-03-10T16:30:18.450700Z","iopub.status.idle":"2023-03-10T16:30:19.773104Z","shell.execute_reply":"2023-03-10T16:30:19.771938Z","shell.execute_reply.started":"2023-03-10T16:30:18.451035Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["_  در حالی که انتشار خبرها درباره تیراندازی به ساختمان یوتیوب ادامه دارد، بررسی اینترنتی سوابق «نسیم نجفی اقدم» مظنون اصلی این حمله حاکی از احتمال «بهایی» بودن او است. بامداد امروز چهارشنبه در ابتدا منابع خبری از تیراندازی در ساختمان مقر مرکزی سایت معروف یوتیوب خبر دادند. ساعاتی پس از انتشار خبر اولیه این تیراندازی، نام «نسیم نجفی اقدم» به عنوان ضارب توسط رسانه‌های آمریکایی اعلام شد (جزئیات بیشتر) بنا بر اعلام رسانه‌های آمریکایی، وی از مواضع و سیاست‌های یوتیوب به شدت انتقاد داشت و پدرش هم گفته بود که او «از یوتیوب متنفر بود». طبق اعلام رسانه‌ها و مقام‌های آمریکایی، ضارب پس از زخمی کردن سه نفر، به خودش تیراندازی و خودکشی کرد. با انجام جستجوهای اینترنتی درباره «نسیم نجفی اقدم»، برخی منابع اطلاعاتی درباره او منتشر کرده‌اند؛ اطلاعاتی که صحت آن قابل تایید و یا تکذیب قطعی نیست. بنا بر اطلاعات برخی از این منابع از قبیل سایت heavy.com، نسیم نجفی اقدم، معتقد به فرقه ضاله «بهائیت» بوده است. او همچنین در برخی از شبکه‌های اجتماعی، مطالبی درباره این فرقه ضاله  هم منتشر و خود را پایبند به آن اعلام کرده بود. سایت «بازفید» هم در این باره گزارشی منتشر کرد. طبق این گزارش که برگرفته از وبسایت خود اقدم است، او از دیدارش با اعضای این فرقه ضاله در سال 2015 خبر داده است. با نگاه به مطالب منتشر شده در شبکه‌های اجتماعی و حتی سایت شخصی او، باید گفت اگر هم وی، خودش عضو فرقه ضاله بهائیت نبوده باشد اما تحت تاثیر تفکرات این فرقه قرار داشت. شبکه اسکای نیوز در گزارشی درباره عامل تیراندازی به ساختمان یوتیوب، جزئیات دیگری منتشر کرد. طبق گزارش این رسانه، اقدم به همراه خانواده خود در سال 1996 میلادی از ایران به آمریکا مهاجرت کرده بود. او به صورت مرتب در یوتیوب پُست‌ها و ویدئوهایی به زبان‌های انگلیسی، فارسی و ترکیه‌ای منتشر می‌کرد\n","-------------\n"]},{"data":{"text/plain":["'در حالی که انتشار خبرها درباره تیراندازی به ساختمان یوتیوب ادامه دارد، بررسی اینترنتی سوابق «نسیم نجفی اقدم» مظنون اصلی این حمله حاکی از احتمال «بهایی» بودن او است.'"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["text = data[0]['body']\n","print(text)\n","print('-------------')\n","summarize(text, model, tokenizer)"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2023-03-07T16:52:06.754685Z","iopub.status.busy":"2023-03-07T16:52:06.754201Z","iopub.status.idle":"2023-03-07T16:52:06.762677Z","shell.execute_reply":"2023-03-07T16:52:06.761624Z","shell.execute_reply.started":"2023-03-07T16:52:06.754640Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['یوتیوب', 'ساختمان یوتیوب', 'نسیم نجفی اقدم', 'بهایی']"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["data[0]['keywords']"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2023-03-07T16:52:09.033724Z","iopub.status.busy":"2023-03-07T16:52:09.033003Z","iopub.status.idle":"2023-03-07T16:52:31.426062Z","shell.execute_reply":"2023-03-07T16:52:31.425042Z","shell.execute_reply.started":"2023-03-07T16:52:09.033683Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["-  /  در حاشیه ششمین اجلاس بین‌المللی نیروهای دریایی کشورهای حاشیه اقیانوس هند که در تهران برگزار شد، ادمیرال ظفر محمود عباسی فرمانده نیروی دریایی پاکستان با دریادار پاسدار علی فدوی فرمانده نیروی دریایی سپاه دیدار و گفت‌وگو کرد. دریادار فدوی در این دیدار با اظهار خرسندی از تعامل و همکاری خوب نیروهای مسلح دو کشور در راستای منافع مشترک، به اوضاع سیاسی و امنیتی منطقه اشاره و تصریح کرد: از آنجا که ایران و پاکستان در یک منطقه قرار دارند، هرگونه ناامنی در منطقه این دو کشور را متأثر خواهد کرد. وی افزود: ناامنی‌های زیادی در منطقه غرب آسیا اتفاق افتاده و عامل همه این ناامنی‌ها آمریکایی‌ها  هستند؛ به گونه‌ای که کشورهای منطقه از جمله ایران و پاکستان ضربات زیادی در اثر تروریسم که منشأ آن آمریکاست خورده‌اند. دریادار فدوی خاطرنشان کرد: خوشبختانه سیاست‌مداران و نظامیان پاکستان موضع‌گیری قاطعانه و متناسب با شرف و اصالت پاکستان برابر حرف‌های نابخردانه ترامپ گرفته‌اند؛ هر چند آمریکا نیز علیه آنها موضع گرفته و اقدام کرده است. فرمانده نیروی دریایی سپاه گفت: از زمان جنگ تحمیلی 8 ساله علیه ایران، آمریکایی‌ها شرارت‌های زیادی داشته‌اند که تعداد بسیار زیادی از آنها  توسط نیروی دریایی سپاه خنثی شده است و در مواردی هم علیه آنها اقدام کرده‌ایم. وی با تاکید بر اینکه سال‌های طولانی است که آمریکا در یک موضع انفعالی و تمکین‌کننده برابر سپاه در خلیج فارس به سر می‌برد، گفت: یک سال و نیم پیش دو شناور آمریکایی توسط نیروی دریایی سپاه توقیف و سرنشینان آن دستگیر شدند و در 40 دقیقه اول ماجرا آمریکا احساس کرد می‌تواند از موضع استکباری حرف بزند و اقداماتی هم انجام داد اما اگر این اقداماتش دقایق دیگری ادامه می‌یافت، قطعا ناوهای آمریکایی مورد اصابت قرار می‌گرفتند. فدوی خاطرنشان کرد: وزیر دفاع آمریکا حدود یک سال پیش در ظفره امارات گفت «ما و متحدین منطقه‌ای یعنی عربستان و امارات و نیز متحدین فرامنطقه‌ای‌مان یعنی اروپایی‌ها به دنبال این هستیم توانی ایجاد کنیم که بتواند در برابر ایران بازدارنده باشد». این در حالی است که آنها مستکبر هستند و خوی استکباری اجازه نمی‌دهد معمولا از این جملات به کار ببرند و این جمله وزیر دفاع آمریکا نشان‌دهنده از قدرت نیروی دریایی سپاه است. فرمانده نیروی دریایی سپاه تصریح کرد: این توان ما صرفا به خاطر قدرت موشکی و شناوری نیست زیرا شناورهای آمریکا قابل مقایسه با هیچ کشوری در دنیا نیست بلکه این توان اسلام و مسلمانان و وعده نصرت الهی است. دریادار فدوی در ادامه تاکید کرد: ایران و پاکستان به عنوان ید واحده اسلام با هم‌افزایی و هماهنگی بیش از پیش می‌توانند بر توان دفاعی خود در حوزه‌های مختلف بویژه دریا بیفزایند. وی با تاکید بر اینکه نیروی دریایی سپاه طبق تعریفی که سردمداران دنیا از قدرت دریایی داشته‌اند، پیش نرفته است و تعریفی متناسب با نیازها و الزامات دفاعی خود دارد که  طبق آن پیش رفته است، یادآور شد: یک بار به فرمانده نیروی دریایی چین گفتم اگر شما طبق تعریف آمریکا نیروی دریایی خود را تجهیز می‌کنید چگونه می‌خواهید با آنها مقابله کنید؟ فرمانده نیروی دریایی سپاه با اشاره به منافع مشترک دو کشور و ابراز امیدواری نسبت به ارتقای مناسبات و همکاری‌های نیروی دریایی جمهوری اسلامی ایران و جمهوری اسلامی پاکستان، اظهار کرد: جمهوری اسلامی ایران دو نیروی دریایی مجزا و در عین حال هم‌افزا دارد و نیروی دریایی پاکستان می‌تواند ارتباطات دریایی خوبی با نیروی دریایی سپاه داشته باشد. فدوی در پایان گفت: امیدواریم با هماهنگی‌های لازم در سال جاری شاهد برگزاری رزمایش مشترک ایران و پاکستان با حضور نیروی دریایی سپاه باشیم. در ادامه این دیدار، ادمیرال ظفرمحمود عباسی فرمانده نیروی دریایی پاکستان با قدردانی از اقدامات جمهوری اسلامی ایران در کمک به تامین امنیت آب‌های منطقه گفت: پاکستان و ایران از گذشته ارتباط تاریخی خوبی براساس اسلام و زبان فارسی با یکدیگر داشته‌اند که متأسفانه با اقدامات انگلیسی‌ها، مردم پاکستان زبان فارسی را فراموش کرده‌اند و به سمت زبان انگلیسی رفته‌اند. وی با بیان اینکه دشمنان به دنبال تفرقه‌‌اندازی هستند، تاکید کرد: پاکستان احترام زیادی به مردم و کشور ایران می‌گذارد؛ چنانکه در زمان جنگ تحمیلی نیز از جمهوری اسلامی ایران حمایت کرد. فرمانده نیروی دریایی پاکستان در پایان با ابراز تاسف از برخی اختلافات میان مسلمانان و تاکید بر اهمیت دستیابی به اتحاد بیشتر و فرهنگ واحد، گفت: امیدوارم در آینده شاهد روابط نظامی نزدیک‌تر دو کشور و میزبان شما در پاکستان باشیم\n","-------------\n"]},{"data":{"text/plain":["'فرمانده نیروی دریایی سپاه گفت: سیاست مداران و نظامیان پاکستان موضع گیری قاطعانه و متناسب با شرف و اصالت پاکستان برابر حرف های نابخردانه ترامپ گرفته اند.'"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["text = data[1]['body']\n","print(text)\n","print('-------------')\n","summarize(text, model, tokenizer)"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2023-03-07T16:45:52.263057Z","iopub.status.busy":"2023-03-07T16:45:52.262768Z","iopub.status.idle":"2023-03-07T16:45:52.269939Z","shell.execute_reply":"2023-03-07T16:45:52.268943Z","shell.execute_reply.started":"2023-03-07T16:45:52.263029Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['رزمایش مشترک', 'نیروهای مسلح', 'نیروی دریایی سپاه', 'دریادار فدوی']"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["data[1]['keywords']"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2023-03-07T17:00:16.776001Z","iopub.status.busy":"2023-03-07T17:00:16.774706Z","iopub.status.idle":"2023-03-07T17:00:16.783245Z","shell.execute_reply":"2023-03-07T17:00:16.782049Z","shell.execute_reply.started":"2023-03-07T17:00:16.775958Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'arshandalili/autotrain-news-summarization-3366493100'"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["model.config.name_or_path"]},{"cell_type":"markdown","metadata":{},"source":["## Persian Main Dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T16:21:53.736336Z","iopub.status.busy":"2023-03-10T16:21:53.735424Z","iopub.status.idle":"2023-03-10T16:21:53.968734Z","shell.execute_reply":"2023-03-10T16:21:53.967652Z","shell.execute_reply.started":"2023-03-10T16:21:53.736282Z"},"trusted":true},"outputs":[],"source":["file_path = \"/kaggle/input/persian-audio-transcript-dataset/persian_dataset.json\"\n","with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","    dataset = json.load(f)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T16:22:05.932241Z","iopub.status.busy":"2023-03-10T16:22:05.931250Z","iopub.status.idle":"2023-03-10T16:22:05.945419Z","shell.execute_reply":"2023-03-10T16:22:05.944042Z","shell.execute_reply.started":"2023-03-10T16:22:05.932192Z"},"trusted":true},"outputs":[{"data":{"text/plain":["dict_keys(['Radio_Marz_-_Episode_27.mp3', 'Radio_Marz_-_Episode_31.mp3', 'Radio_Marz_-_Episode_38.mp3', 'Radio_Marz_-_Episode_15.mp3', 'Radio_Marz_-_Episode_30_-_Edited.mp3', 'Radio_Marz_-_Episode_44.mp3', 'Radio_Marz_-_Episode_43.mp3', 'Radio_Marz_-_Episode_36.mp3', 'Radio_Marz_-_Episode_46.mp3', 'Radio_Marz_-_Episode_32.mp3', 'Radio_Marz_-_Episode_33.mp3', 'Radio_Marz_-_Episode_22.mp3', 'Radio_Marz_-_Episode_45.mp3', 'Radio_Marz_-_Episode_51.mp3', 'Radio_Marz_-_Episode_39.mp3', 'Radio_Marz_-_Episode_26.mp3', 'Radio_Marz_-_Episode_48.mp3', 'Radio_Marz_-_Episode_19.mp3', 'Radio_Marz-_Episode_8.mp3', 'Radio_Marz_-_Episode_41.mp3', 'Radio_Marz_-_Episode_17_2.mp3', 'Radio_Marz_-_Episode_29.mp3', 'Radio_Marz_-_Episode_37.mp3', 'Radio_Marz_-_Episode_23.mp3', 'Radio_Marz_-_Episode_24.mp3', 'Radio_Marz_-_Episode_34.mp3', 'Radio_Marz_-_Episode_11.mp3', 'Radio_Marz_-_Episode_16_-_biodent.mp3', 'Radio_Marz_-_Episode_13.mp3', 'Radio_Marz_-_Episode_12.mp3', 'Radio_Marz_-_Episode_20.mp3', 'Radio_Marz_-_Episode_35.mp3', 'Radio_Marz_-_Episode_47.mp3', 'Radio_Marz-_Episode_9.mp3', 'Radio_Marz_-_Episode_50_-_NewEdited.mp3', 'Radio_Marz_-_Episode_49.mp3', 'Radio_Marz_-_Episode_18.mp3', 'Radio_Marz-_Episode_10.mp3', 'Radio_Marz_-_Episode_42.mp3', 'Radio_Marz_-_Episode_21.mp3', 'Radio_Marz_-_Episode_40.mp3', 'Radio_Marz_-Episode_28.mp3'])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["dataset.keys()"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T16:22:48.483065Z","iopub.status.busy":"2023-03-10T16:22:48.482682Z","iopub.status.idle":"2023-03-10T16:22:48.489962Z","shell.execute_reply":"2023-03-10T16:22:48.488937Z","shell.execute_reply.started":"2023-03-10T16:22:48.483032Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'start_time': 0.0,\n"," 'end_time': 13.65,\n"," 'transcription': 'اونجا که حسی یه سر انگار نمی دونم فانطزیتر آدم فکر می کنه میگین میایی بیرون همه چیناسم گل و بل بل همه چیز عکی می ریب پیش خونهواده اندامی رو رابطه سمیمی ولی حسم وقتی اومدم بیرون هاستن استقریب بگی'}"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["dataset['Radio_Marz_-_Episode_22.mp3'][0]"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T16:31:40.226109Z","iopub.status.busy":"2023-03-10T16:31:40.225396Z","iopub.status.idle":"2023-03-10T16:31:40.230660Z","shell.execute_reply":"2023-03-10T16:31:40.229571Z","shell.execute_reply.started":"2023-03-10T16:31:40.226071Z"},"trusted":true},"outputs":[],"source":["gc.enable()"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T16:36:59.459763Z","iopub.status.busy":"2023-03-10T16:36:59.458831Z","iopub.status.idle":"2023-03-10T16:36:59.466627Z","shell.execute_reply":"2023-03-10T16:36:59.465597Z","shell.execute_reply.started":"2023-03-10T16:36:59.459714Z"},"trusted":true},"outputs":[{"data":{"text/plain":["42"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["len(dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T16:43:04.205653Z","iopub.status.busy":"2023-03-10T16:43:04.204630Z"},"trusted":true},"outputs":[],"source":["new_dataset = dict()\n","j = 1\n","for key, value in dataset.items():\n","    new_value = []\n","    with tqdm(enumerate(value), total=len(value)) as pbar:\n","        for i, item in pbar:\n","            try:\n","                if i % 10 == 0:\n","                    torch.cuda.empty_cache()\n","                    torch.cuda.ipc_collect()\n","                    gc.collect()\n","                text = item['transcription']\n","                summarized = summarize(text, model, tokenizer)\n","                item['keywords'] = summarized\n","            except Exception as e:\n","                print(e)\n","                continue\n","            new_value.append(item)\n","    new_dataset[key] = new_value\n","    with open(f'dataset_v{j}', \"w\", encoding=\"utf-8\") as f:\n","        dataset = json.dump(new_dataset, f)\n","    j += 1"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
